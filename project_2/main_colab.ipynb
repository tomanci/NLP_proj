{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJ7hy85CfWa26c5ohrPojK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import random as rd\n",
        "import numpy as np\n",
        "\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "#path_dataset = \"/Users/tommasoancilli/Downloads/ita-eng/ita.txt\"\n",
        "\n",
        "def Text_creation (path_dataset, epsilon, max_length = 10):\n",
        "\n",
        "   data = pd.read_csv(path_dataset, header=None, delimiter = \"\\t\")\n",
        "\n",
        "   EPSILON = epsilon # decide the fraction of senteces to be included in the dataset\n",
        "   MAX_LENGTH = max_length  #max length of sentences allowed\n",
        "\n",
        "   trans_file_eng = open(\"eng.txt\", \"w\")\n",
        "   trans_file_ita = open(\"ita.txt\", \"w\")\n",
        "\n",
        "   for item in range(data.shape[0]):\n",
        "      text_it = data[1][item]\n",
        "      text_eng =  data[0][item]\n",
        "\n",
        "      text_it_split = text_it.split()\n",
        "      text_eng_split = text_eng.split()\n",
        "\n",
        "      if len(text_it_split) <= MAX_LENGTH and len(text_eng_split) <= MAX_LENGTH:\n",
        "         if rd.random() < EPSILON:\n",
        "            trans_file_eng.write(text_eng + \"\\n\")\n",
        "            trans_file_ita.write(text_it + \"\\n\")\n",
        "\n",
        "   trans_file_eng.close()\n",
        "   trans_file_ita.close()\n",
        "\n",
        "class Language ():\n",
        "\n",
        "   def __init__(self, path_name) -> None:\n",
        "\n",
        "      self.path_name = path_name\n",
        "\n",
        "      self.word2index = {\"EOS\":0, \"SOS\":1}\n",
        "      self.index2word = {0:\"EOS\", 1:\"SOS\"}\n",
        "      self.n_words = 2\n",
        "      self.n_sentences = 0\n",
        "      self.n_max_length = 0\n",
        "\n",
        "   def normalize_string (self, s) -> str:\n",
        "      s = re.sub(r\"([.!? \\, \\' \\\" \\% \\-])\", r\" \", s) #remove puntuation\n",
        "      s = s.lower() #convert to lower\n",
        "      s = s.strip() # remove spaces from the beginning / end\n",
        "      #s = \"SOS\" + \" \" + s + \" \"+ \"EOS\"\n",
        "      s = s + \" \" \"EOS\"\n",
        "      return s\n",
        "\n",
        "   def add_string(self, s):\n",
        "      partial_length = 0\n",
        "      for word in s.split(\" \"):\n",
        "         self.add_word(word)\n",
        "         partial_length = partial_length + 1\n",
        "\n",
        "      self.n_sentences = self.n_sentences + 1\n",
        "\n",
        "      if self.n_max_length < partial_length:\n",
        "         self.n_max_length = partial_length\n",
        "\n",
        "   def add_word(self, word):\n",
        "\n",
        "      if word not in self.word2index:\n",
        "         self.word2index[word] = self.n_words\n",
        "         self.index2word[self.n_words] = word\n",
        "         self.n_words = self.n_words + 1\n",
        "\n",
        "   def string_processing(self):\n",
        "      processed_file = open(\"processed-\"+self.path_name, \"w\")\n",
        "\n",
        "      with open(str(self.path_name)) as file:\n",
        "         for s in file:\n",
        "            s = self.normalize_string(s)\n",
        "            processed_file.write(s +\"\\n\")\n",
        "            self.add_string(s)\n",
        "\n",
        "      processed_file.close()\n",
        "\n",
        "   def string_translation(self, input_string):\n",
        "\n",
        "      input_string = self.normalize_string(input_string)\n",
        "      if len(input_string.split()) > self.n_max_length:\n",
        "         raise KeyError(\"LENGTH OF THE INPUT SENTENCE IS TOO LONG!!\")\n",
        "\n",
        "      matrix  = np.zeros( (self.n_max_length) )\n",
        "\n",
        "\n",
        "      for idx, word in enumerate(input_string.split(), start = 0):\n",
        "            if word in self.word2index:\n",
        "               matrix[idx] = self.word2index[word]\n",
        "            else:\n",
        "               raise KeyError(\"WORD NOT FOUND, IMPOSSIBLE TO TRANSLATE\")\n",
        "\n",
        "      np.save(f'matrix-processed.npy', matrix)\n",
        "\n",
        "      return matrix\n",
        "\n",
        "\n",
        "   #TODO #2:\n",
        "\n",
        "\n",
        "def Dataset_creation(lang_input, lang_output, training_test_ratio:tuple = (0.85,0.15)):\n",
        "\n",
        "   MAX_LENGTH = max(lang_input.n_max_length, lang_output.n_max_length) + 1 # I've -> I ve so I have two words now\n",
        "\n",
        "   if lang_input.n_sentences != lang_output.n_sentences:\n",
        "      raise KeyError (\"Error, the number of examples does not match\")\n",
        "\n",
        "   #TODO #1:\n",
        "   matrix_input = Matrix_creation(lang_input, MAX_LENGTH)\n",
        "   matrix_output = Matrix_creation(lang_output, MAX_LENGTH)\n",
        "\n",
        "   split_train_test(matrix_input, matrix_output, training_test_ratio)\n",
        "\n",
        "\n",
        "def Matrix_creation(lang, MAX_LENGTH):\n",
        "   matrix  = np.zeros( (lang.n_sentences, MAX_LENGTH) )\n",
        "   path_input = \"processed-\"+lang.path_name\n",
        "\n",
        "   try:\n",
        "\n",
        "      with open(str(path_input), 'r') as file:\n",
        "         for idx, line in enumerate(file, start = 0):\n",
        "            words = line.split()\n",
        "            for id, word in enumerate(words, start = 0 ):\n",
        "               matrix[idx][id] = lang.word2index[word]\n",
        "\n",
        "   except Exception as e: #TODO #3\n",
        "      print(f\"{e},{words},{idx},{id}\")\n",
        "\n",
        "   np.save(f'matrix-{re.sub(\".txt\", \"\", lang.path_name)}.npy', matrix)\n",
        "   return matrix\n",
        "\n",
        "\n",
        "def main ():\n",
        "   path_dataset = \"/Users/tommasoancilli/Downloads/ita-eng/ita.txt\"\n",
        "   max_length = 10\n",
        "\n",
        "   Text_creation (path_dataset, epsilon, max_length = max_length)\n",
        "\n",
        "   input_lang = Language(path_name = \"eng.txt\")\n",
        "   output_lang = Language(path_name = \"ita.txt\")\n",
        "\n",
        "   input_lang.string_processing()\n",
        "   output_lang.string_processing()\n",
        "\n",
        "   Dataset_creation(input_lang, output_lang,training_test_ratio= (0.85,0.15))\n",
        "\n",
        "\n",
        "def split_train_test(matrix_input, matrix_target, training_test_ratio:tuple = (0.85,0.15)):\n",
        "\n",
        "   dim = matrix_input.shape\n",
        "   n_rows = dim[0]\n",
        "\n",
        "   input_train = []\n",
        "   output_train = []\n",
        "\n",
        "   input_test = []\n",
        "   output_test = []\n",
        "\n",
        "   shuffled_list = rd.sample(range(n_rows), n_rows)  # The range of integers from 0 to n-1\n",
        "\n",
        "   n_example_training = int(n_rows * training_test_ratio[0])\n",
        "   n_example_test = n_rows - n_example_training\n",
        "\n",
        "   for i in range(n_example_training):\n",
        "      item = shuffled_list[i]\n",
        "      input_train.append( matrix_input[item] )\n",
        "      output_train.append( matrix_target[item] )\n",
        "\n",
        "   for i in range(n_example_training,n_rows):\n",
        "      item = shuffled_list[i]\n",
        "      input_test.append( matrix_input[item] )\n",
        "      output_test.append( matrix_target[item] )\n",
        "\n",
        "   np.save('input_train.npy', input_train)\n",
        "   np.save('input_test.npy', input_test)\n",
        "\n",
        "   np.save('output_train.npy', output_train)\n",
        "   np.save('output_test.npy', output_test)\n",
        "\n",
        "   return 0\n",
        "\n",
        "\"\"\"\n",
        "if __name__ == \"__main__\":\n",
        "   main()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "X4Yg_Q4MAMhc",
        "outputId": "f14204a9-81e2-4f06-8205-0ad8ac2edc3e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nif __name__ == \"__main__\":\\n   main()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "SOS = 1\n",
        "EOS = 0\n",
        "class EncoderRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, device, hidden_size, num_layers, dropout_p=0.1, emb = True):\n",
        "\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.device = device\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size).to(self.device)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers=num_layers,  batch_first=True).to(self.device)\n",
        "        self.dropout = nn.Dropout(dropout_p).to(self.device)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, num_layers, device):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.device = device\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        # analizza ogni riga della matrice, per tutta la sua lunghezza\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=self.device).fill_(SOS)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "\n",
        "        if target_tensor is not None:\n",
        "          dim = target_tensor.shape[1]\n",
        "        else:\n",
        "          dim = max_length + 1\n",
        "\n",
        "        for i in range(dim):\n",
        "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
        "\n",
        "    def forward_step(self, input, hidden):\n",
        "        output = self.embedding(input)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.out(output)\n",
        "        return output, hidden\n",
        "\n",
        "def numpy2torch(numpy_path):\n",
        "    \"\"\"\n",
        "    It converts the numpy matrix into a torch tensor to process the data\n",
        "    \"\"\"\n",
        "    matrix = np.load(numpy_path)\n",
        "\n",
        "    return matrix\n",
        "\n",
        "def torch_format(batch_size, input_train, output_train, device):\n",
        "\n",
        "  train_data = TensorDataset(torch.LongTensor(input_train).to(device), torch.LongTensor(output_train).to(device))\n",
        "  #train_sampler = RandomSampler(train_data)\n",
        "  #train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "  train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "\n",
        "  return train_dataloader\n",
        "\n",
        "\n",
        "def train_epoch(encoder, decoder, n_elem_batch, learning_rate, train_dataloader, loss_function, device):\n",
        "    \"\"\"\n",
        "    training funtion on a single epoch of the input matrix dataset\n",
        "    \"\"\"\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    total_loss_batch = 0\n",
        "\n",
        "    for data in train_dataloader:\n",
        "\n",
        "      encoder_optimizer.zero_grad()\n",
        "      decoder_optimizer.zero_grad()\n",
        "\n",
        "      input_tensor, output_tensor = data\n",
        "\n",
        "      encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "      decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, output_tensor)\n",
        "\n",
        "      loss = loss_function(\n",
        "            decoder_outputs.view(-1, decoder_outputs.size(-1)).to(device),\n",
        "            output_tensor.view(-1).to(device) )\n",
        "\n",
        "      loss.backward()\n",
        "      total_loss_batch += loss.item()\n",
        "\n",
        "      encoder_optimizer.step()\n",
        "      decoder_optimizer.step()\n",
        "      #print(input_tensor.shape)\n",
        "      #print(f\"total loss{total_loss_batch}, dimension{input_tensor.shape}\") #-> da sostituire con dimensioni\n",
        "    return total_loss_batch/input_tensor.shape[0]\n",
        "\n",
        "\n",
        "def train(encoder, decoder, n_elem_batch, learning_rate, train_dataloader, n_epochs, device):\n",
        "\n",
        "    loss_function = nn.NLLLoss()\n",
        "    total_train_loss_plot = []\n",
        "\n",
        "    # Da aggiungere i plot se ci interessano =)\n",
        "\n",
        "    for steps in tqdm( list (range(n_epochs)), desc=\"number of epochs\"):\n",
        "\n",
        "        error = train_epoch(encoder, decoder, n_elem_batch, learning_rate, train_dataloader, loss_function, device)\n",
        "        total_train_loss_plot.append(error)\n",
        "\n",
        "    plt.plot(total_train_loss_plot)\n",
        "\n",
        "def test(encoder, decoder, test_dataloader, device):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    loss_function = nn.NLLLoss()\n",
        "    test_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for data in test_dataloader:\n",
        "          input_tensor, output_tensor = data\n",
        "          encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "          decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, output_tensor)\n",
        "\n",
        "          test_loss += loss_function(\n",
        "          decoder_outputs.view(-1, decoder_outputs.size(-1)).to(device),\n",
        "          output_tensor.view(-1).to(device))\n",
        "\n",
        "          #print(input_tensor[0],output_tensor[0])\n",
        "      #print(test_loss)\n",
        "      return test_loss / decoder_outputs.shape[0]\n",
        "\n",
        "def translation(encoder, decoder, input_lang, output_lang,device):\n",
        "\n",
        "    input_sentence = input(\"Type the sentence you want to translate:\")\n",
        "    vector = input_lang.string_translation(input_sentence)\n",
        "    input_tensor = torch.tensor(vector,dtype=torch.long).to(device).unsqueeze(1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "\n",
        "        #print(decoded_ids)\n",
        "        #print(decoded_ids.shape)\n",
        "        for i in range(3):\n",
        "          decoded_words = []\n",
        "          print(f\"{i} solution proposed out of 3\")\n",
        "          for idx in decoded_ids[i]:\n",
        "            if idx.item() == EOS:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            decoded_words.append(output_lang.index2word[idx.item()])\n",
        "\n",
        "          print(f\"Original sentence = {input_sentence} --> Translated sentence = {decoded_words}\")\n",
        "\n",
        "\n",
        "def evaluation(encoder, decoder, n_elem_batch, learning_rate, n_epochs, train_dataloader, device, test_dataloader):\n",
        "\n",
        "    train(encoder, decoder, n_elem_batch, learning_rate, train_dataloader, n_epochs,device)\n",
        "    test(encoder, decoder, test_dataloader, device)\n",
        "\n"
      ],
      "metadata": {
        "id": "onN7TMVf2mVt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from text_process import *\n",
        "#from RNN import *\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "path_dataset = \"/content/ita2eng.txt\"\n",
        "max_length = 10\n",
        "epsilon = 0.3\n",
        "\n",
        "Text_creation(path_dataset, epsilon = epsilon, max_length = max_length)\n",
        "input_lang = Language(path_name = \"eng.txt\")\n",
        "output_lang = Language(path_name = \"ita.txt\")\n",
        "\n",
        "input_lang.string_processing()\n",
        "output_lang.string_processing()\n",
        "\n",
        "Dataset_creation(input_lang, output_lang,training_test_ratio= (0.8,0.2))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mA2zJIhjAhY",
        "outputId": "d96b8d68-9b5e-4e92-887b-c31d58904522"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 512\n",
        "batch_size = 64\n",
        "number_of_epochs = 128\n",
        "number_layers = 4\n",
        "\n",
        "input_train = numpy2torch(\"input_train.npy\")\n",
        "output_train = numpy2torch(\"output_train.npy\")\n",
        "\n",
        "input_test = numpy2torch(\"input_test.npy\")\n",
        "output_test = numpy2torch(\"output_test.npy\")\n",
        "\n",
        "Encoder = EncoderRNN(input_size=input_lang.n_words, device=device, hidden_size=hidden_size, num_layers=number_layers).to(device)\n",
        "Decoder = DecoderRNN(hidden_size=hidden_size, device = device, output_size=output_lang.n_words, num_layers=number_layers).to(device)\n",
        "\n",
        "train_dataloader = torch_format(batch_size, input_train, output_train, device)\n",
        "test_dataloader = torch_format(batch_size, input_test, output_test, device)\n",
        "\n",
        "evaluation(encoder=Encoder, decoder=Decoder, n_elem_batch=batch_size, learning_rate=0.001, n_epochs=number_of_epochs,\n",
        "           train_dataloader = train_dataloader, test_dataloader = test_dataloader, device= device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxBBCAPhkOCl",
        "outputId": "48618455-c649-4d74-a050-b5836769a2a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "number of epochs:  12%|█▎        | 16/128 [56:44<6:28:03, 207.89s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  translation(encoder=Encoder, decoder=Decoder, input_lang=input_lang, output_lang=output_lang, device=device)\n",
        "  response = input(\"do you want to translate other sentences? [Y/N]: \")\n",
        "  if response == \"N\":\n",
        "    break"
      ],
      "metadata": {
        "id": "rVlrXOWf286b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}