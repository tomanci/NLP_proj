{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNESO7j1SLnSsuEiqsK2jHy"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import random as rd\n",
        "import numpy as np\n",
        "\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "#path_dataset = \"/Users/tommasoancilli/Downloads/ita-eng/ita.txt\"\n",
        "\n",
        "def Text_creation (path_dataset, epsilon = 0.3, max_length = 10):\n",
        "\n",
        "   data = pd.read_csv(path_dataset, header=None, delimiter = \"\\t\")\n",
        "\n",
        "   EPSILON = epsilon # decide the fraction of senteces to be included in the dataset\n",
        "   MAX_LENGTH = max_length  #max length of sentences allowed\n",
        "\n",
        "   trans_file_eng = open(\"eng.txt\", \"w\")\n",
        "   trans_file_ita = open(\"ita.txt\", \"w\")\n",
        "\n",
        "   for item in range(data.shape[0]):\n",
        "      text_it = data[1][item]\n",
        "      text_eng =  data[0][item]\n",
        "\n",
        "      text_it_split = text_it.split()\n",
        "      text_eng_split = text_eng.split()\n",
        "\n",
        "      if len(text_it_split) <= MAX_LENGTH and len(text_eng_split) <= MAX_LENGTH:\n",
        "         if rd.random() < EPSILON:\n",
        "            trans_file_eng.write(text_eng + \"\\n\")\n",
        "            trans_file_ita.write(text_it + \"\\n\")\n",
        "\n",
        "   trans_file_eng.close()\n",
        "   trans_file_ita.close()\n",
        "\n",
        "class Language ():\n",
        "\n",
        "   def __init__(self, path_name) -> None:\n",
        "\n",
        "      self.path_name = path_name\n",
        "\n",
        "      self.word2index = {\"EOS\":0, \"SOS\":1}\n",
        "      self.index2word = {0:\"EOS\", 1:\"SOS\"}\n",
        "      self.n_words = 2\n",
        "      self.n_sentences = 0\n",
        "      self.n_max_length = 0\n",
        "\n",
        "   def normalize_string (self, s) -> str:\n",
        "      s = re.sub(r\"([.!? \\, \\' \\\" \\% \\-])\", r\" \", s) #remove puntuation\n",
        "      s = s.lower() #convert to lower\n",
        "      s = s.strip() # remove spaces from the beginning / end\n",
        "      #s = \"SOS\" + \" \" + s + \" \"+ \"EOS\"\n",
        "      s = s + \" \" \"EOS\"\n",
        "      return s\n",
        "\n",
        "   def add_string(self, s):\n",
        "      partial_length = 0\n",
        "      for word in s.split(\" \"):\n",
        "         self.add_word(word)\n",
        "         partial_length = partial_length + 1\n",
        "\n",
        "      self.n_sentences = self.n_sentences + 1\n",
        "\n",
        "      if self.n_max_length < partial_length:\n",
        "         self.n_max_length = partial_length\n",
        "\n",
        "   def add_word(self, word):\n",
        "\n",
        "      if word not in self.word2index:\n",
        "         self.word2index[word] = self.n_words\n",
        "         self.index2word[self.n_words] = word\n",
        "         self.n_words = self.n_words + 1\n",
        "\n",
        "   def string_processing(self):\n",
        "      processed_file = open(\"processed-\"+self.path_name, \"w\")\n",
        "\n",
        "      with open(str(self.path_name)) as file:\n",
        "         for s in file:\n",
        "            s = self.normalize_string(s)\n",
        "            processed_file.write(s +\"\\n\")\n",
        "            self.add_string(s)\n",
        "\n",
        "      processed_file.close()\n",
        "\n",
        "   def string_translation(self, input_string):\n",
        "\n",
        "      input_string = self.normalize_string(input_string)\n",
        "      if len(input_string.split()) > self.n_max_length:\n",
        "         raise KeyError(\"LENGTH OF THE INPUT SENTENCE IS TOO LONG!!\")\n",
        "\n",
        "      matrix  = np.zeros( (self.n_max_length) )\n",
        "\n",
        "\n",
        "      for idx, word in enumerate(input_string.split(), start = 0):\n",
        "            if word in self.word2index:\n",
        "               matrix[idx] = self.word2index[word]\n",
        "            else:\n",
        "               raise KeyError(\"WORD NOT FOUND, IMPOSSIBLE TO TRANSLATE\")\n",
        "\n",
        "      np.save(f'matrix-processed.npy', matrix)\n",
        "\n",
        "      return matrix\n",
        "\n",
        "\n",
        "   #TODO #2:\n",
        "\n",
        "\n",
        "def Dataset_creation(lang_input, lang_output, training_test_ratio:tuple = (0.85,0.05,0.1)):\n",
        "\n",
        "   MAX_LENGTH = max(lang_input.n_max_length, lang_output.n_max_length) + 1 # I've -> I ve so I have two words now\n",
        "\n",
        "   if lang_input.n_sentences != lang_output.n_sentences:\n",
        "      raise KeyError (\"Error, the number of examples does not match\")\n",
        "\n",
        "   #TODO #1:\n",
        "   matrix_input = Matrix_creation(lang_input, MAX_LENGTH)\n",
        "   matrix_output = Matrix_creation(lang_output, MAX_LENGTH)\n",
        "\n",
        "   split_train_test(matrix_input, matrix_output, training_test_ratio)\n",
        "\n",
        "\n",
        "def Matrix_creation(lang, MAX_LENGTH):\n",
        "   matrix  = np.zeros( (lang.n_sentences, MAX_LENGTH) )\n",
        "   path_input = \"processed-\"+lang.path_name\n",
        "\n",
        "   try:\n",
        "\n",
        "      with open(str(path_input), 'r') as file:\n",
        "         for idx, line in enumerate(file, start = 0):\n",
        "            words = line.split()\n",
        "            for id, word in enumerate(words, start = 0 ):\n",
        "               matrix[idx][id] = lang.word2index[word]\n",
        "\n",
        "   except Exception as e: #TODO #3\n",
        "      print(f\"{e},{words},{idx},{id}\")\n",
        "\n",
        "   np.save(f'matrix-{re.sub(\".txt\", \"\", lang.path_name)}.npy', matrix)\n",
        "   return matrix\n",
        "\n",
        "\n",
        "def main ():\n",
        "   path_dataset = \"/Users/tommasoancilli/Downloads/ita-eng/ita.txt\"\n",
        "   max_length = 10\n",
        "\n",
        "   Text_creation (path_dataset, epsilon = 0.3, max_length = max_length)\n",
        "\n",
        "   input_lang = Language(path_name = \"eng.txt\")\n",
        "   output_lang = Language(path_name = \"ita.txt\")\n",
        "\n",
        "   input_lang.string_processing()\n",
        "   output_lang.string_processing()\n",
        "\n",
        "   Dataset_creation(input_lang, output_lang,training_test_ratio= (0.85,0.15))\n",
        "\n",
        "\n",
        "def split_train_test(matrix_input, matrix_target, training_test_ratio:tuple = (0.85,0.15)):\n",
        "\n",
        "   dim = matrix_input.shape\n",
        "   n_rows = dim[0]\n",
        "\n",
        "   input_train = []\n",
        "   output_train = []\n",
        "\n",
        "   input_test = []\n",
        "   output_test = []\n",
        "\n",
        "   shuffled_list = rd.sample(range(n_rows), n_rows)  # The range of integers from 0 to n-1\n",
        "\n",
        "   n_example_training = int(n_rows * training_test_ratio[0])\n",
        "   n_example_test = n_rows - n_example_training\n",
        "\n",
        "   for i in range(n_example_training):\n",
        "      item = shuffled_list[i]\n",
        "      input_train.append( matrix_input[item] )\n",
        "      output_train.append( matrix_target[item] )\n",
        "\n",
        "   for i in range(n_example_training,n_rows):\n",
        "      item = shuffled_list[i]\n",
        "      input_test.append( matrix_input[item] )\n",
        "      output_test.append( matrix_target[item] )\n",
        "\n",
        "   np.save('input_train.npy', input_train)\n",
        "   np.save('input_test.npy', input_test)\n",
        "\n",
        "   np.save('output_train.npy', output_train)\n",
        "   np.save('output_test.npy', output_test)\n",
        "\n",
        "   return 0\n",
        "\n",
        "\"\"\"\n",
        "if __name__ == \"__main__\":\n",
        "   main()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "X4Yg_Q4MAMhc",
        "outputId": "c528dbe8-0e99-43de-a1ec-b6258b916a64"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nif __name__ == \"__main__\":\\n   main()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "SOS = 1\n",
        "EOS = 0\n",
        "class EncoderRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, device, hidden_size, dropout_p=0.1, emb = True):\n",
        "\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.device = device\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size).to(self.device)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True).to(self.device)\n",
        "        self.dropout = nn.Dropout(dropout_p).to(self.device)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, device):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.device = device\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        # analizza ogni riga della matrice, per tutta la sua lunghezza\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=self.device).fill_(SOS)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "\n",
        "        if target_tensor is not None:\n",
        "          dim = target_tensor.shape[1]\n",
        "        else:\n",
        "          dim = max_length + 1\n",
        "\n",
        "        for i in range(dim):\n",
        "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
        "\n",
        "    def forward_step(self, input, hidden):\n",
        "        output = self.embedding(input)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.out(output)\n",
        "        return output, hidden\n",
        "\n",
        "def numpy2torch(numpy_path):\n",
        "    \"\"\"\n",
        "    It converts the numpy matrix into a torch tensor to process the data\n",
        "    \"\"\"\n",
        "    matrix = np.load(numpy_path)\n",
        "\n",
        "    return matrix\n",
        "\n",
        "def torch_format(batch_size, input_train, output_train, device):\n",
        "\n",
        "  train_data = TensorDataset(torch.LongTensor(input_train).to(device), torch.LongTensor(output_train).to(device))\n",
        "  #train_sampler = RandomSampler(train_data)\n",
        "  #train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "  train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "\n",
        "  return train_dataloader\n",
        "\n",
        "\n",
        "def train_epoch(encoder, decoder, n_elem_batch, learning_rate, train_dataloader, loss_function, device):\n",
        "    \"\"\"\n",
        "    training funtion on a single epoch of the input matrix dataset\n",
        "    \"\"\"\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    total_loss_batch = 0\n",
        "\n",
        "    for data in train_dataloader:\n",
        "\n",
        "      encoder_optimizer.zero_grad()\n",
        "      decoder_optimizer.zero_grad()\n",
        "\n",
        "      input_tensor, output_tensor = data\n",
        "\n",
        "      encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "      decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, output_tensor)\n",
        "\n",
        "      loss = loss_function(\n",
        "            decoder_outputs.view(-1, decoder_outputs.size(-1)).to(device),\n",
        "            output_tensor.view(-1).to(device) )\n",
        "\n",
        "      loss.backward()\n",
        "      total_loss_batch += loss.item()\n",
        "\n",
        "      encoder_optimizer.step()\n",
        "      decoder_optimizer.step()\n",
        "      print(input_tensor.shape)\n",
        "      #print(f\"total loss{total_loss_batch}, dimension{input_tensor.shape}\") #-> da sostituire con dimensioni\n",
        "    return total_loss_batch/input_tensor.shape[0]\n",
        "\n",
        "\n",
        "def train(encoder, decoder, n_elem_batch, learning_rate, train_dataloader, n_epochs, device):\n",
        "\n",
        "    loss_function = nn.NLLLoss()\n",
        "    total_train_loss_plot = []\n",
        "\n",
        "    # Da aggiungere i plot se ci interessano =)\n",
        "\n",
        "    for steps in tqdm( list (range(n_epochs)), desc=\"number of epochs\"):\n",
        "\n",
        "        error = train_epoch(encoder, decoder, n_elem_batch, learning_rate, train_dataloader, loss_function, device)\n",
        "        total_train_loss_plot.append(error)\n",
        "\n",
        "    plt.plot(total_train_loss_plot)\n",
        "\n",
        "def test(encoder, decoder, test_dataloader, device):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    loss_function = nn.NLLLoss()\n",
        "    test_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for data in test_dataloader:\n",
        "          input_tensor, output_tensor = data\n",
        "          encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "          decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, output_tensor)\n",
        "\n",
        "          test_loss += loss_function(\n",
        "          decoder_outputs.view(-1, decoder_outputs.size(-1)).to(device),\n",
        "          output_tensor.view(-1).to(device))\n",
        "\n",
        "          #print(input_tensor[0],output_tensor[0])\n",
        "      #print(test_loss)\n",
        "      return test_loss / decoder_outputs.shape[0]\n",
        "\n",
        "def translation(encoder, decoder, input_lang, output_lang,device):\n",
        "\n",
        "    input_sentence = input(\"Type the sentence you want to translate:\")\n",
        "    vector = input_lang.string_translation(input_sentence)\n",
        "    input_tensor = torch.tensor(vector,dtype=torch.long).to(device).unsqueeze(1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "\n",
        "        #print(decoded_ids)\n",
        "        #print(decoded_ids.shape)\n",
        "        for i in range(3):\n",
        "          decoded_words = []\n",
        "          print(f\"{i} solution proposed out of 3\")\n",
        "          for idx in decoded_ids[i]:\n",
        "            if idx.item() == EOS:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            decoded_words.append(output_lang.index2word[idx.item()])\n",
        "\n",
        "          print(f\"Original sentence = {input_sentence} --> Translated sentence = {decoded_words}\")\n",
        "\n",
        "\n",
        "def evaluation(encoder, decoder, n_elem_batch, learning_rate, n_epochs, train_dataloader, device, test_dataloader):\n",
        "\n",
        "    train(encoder, decoder, n_elem_batch, learning_rate, train_dataloader, n_epochs,device)\n",
        "    test(encoder, decoder, test_dataloader, device)\n",
        "\n"
      ],
      "metadata": {
        "id": "onN7TMVf2mVt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from text_process import *\n",
        "#from RNN import *\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "path_dataset = \"/content/ita2eng.txt\"\n",
        "max_length = 10\n",
        "\n",
        "Text_creation(path_dataset, epsilon = 0.3, max_length = max_length)\n",
        "input_lang = Language(path_name = \"eng.txt\")\n",
        "output_lang = Language(path_name = \"ita.txt\")\n",
        "\n",
        "input_lang.string_processing()\n",
        "output_lang.string_processing()\n",
        "\n",
        "Dataset_creation(input_lang, output_lang,training_test_ratio= (0.8,0.2))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mA2zJIhjAhY",
        "outputId": "836e254e-950f-4414-dd85-f86f5fc43cc2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 1024\n",
        "batch_size = 64\n",
        "number_of_epochs = 256\n",
        "\n",
        "input_train = numpy2torch(\"input_train.npy\")\n",
        "output_train = numpy2torch(\"output_train.npy\")\n",
        "\n",
        "input_test = numpy2torch(\"input_test.npy\")\n",
        "output_test = numpy2torch(\"output_test.npy\")\n",
        "\n",
        "Encoder = EncoderRNN(input_size=input_lang.n_words, device=device, hidden_size=hidden_size).to(device)\n",
        "Decoder = DecoderRNN(hidden_size=hidden_size, device = device, output_size=output_lang.n_words).to(device)\n",
        "\n",
        "train_dataloader = torch_format(batch_size, input_train, output_train, device)\n",
        "test_dataloader = torch_format(batch_size, input_test, output_test, device)\n",
        "\n",
        "evaluation(encoder=Encoder, decoder=Decoder, n_elem_batch=batch_size, learning_rate=0.001, n_epochs=number_of_epochs,\n",
        "           train_dataloader = train_dataloader, test_dataloader = test_dataloader, device= device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "XxBBCAPhkOCl",
        "outputId": "6c457f9b-a123-4ceb-9396-d56271178938"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnumber of epochs:   0%|          | 0/256 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 10])\n",
            "torch.Size([64, 10])\n",
            "torch.Size([64, 10])\n",
            "torch.Size([64, 10])\n",
            "torch.Size([64, 10])\n",
            "torch.Size([64, 10])\n",
            "torch.Size([64, 10])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnumber of epochs:   0%|          | 0/256 [00:14<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2287cd6a4b25>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m evaluation(encoder=Encoder, decoder=Decoder, n_elem_batch=batch_size, learning_rate=0.001, n_epochs=number_of_epochs,\n\u001b[0m\u001b[1;32m     18\u001b[0m            train_dataloader = train_dataloader, test_dataloader = test_dataloader, device= device)\n",
            "\u001b[0;32m<ipython-input-2-ee3536562eab>\u001b[0m in \u001b[0;36mevaluation\u001b[0;34m(encoder, decoder, n_elem_batch, learning_rate, n_epochs, train_dataloader, device, test_dataloader)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_elem_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_elem_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-ee3536562eab>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, n_elem_batch, learning_rate, train_dataloader, n_epochs, device)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mlist\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"number of epochs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_elem_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0mtotal_train_loss_plot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-ee3536562eab>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(encoder, decoder, n_elem_batch, learning_rate, train_dataloader, loss_function, device)\u001b[0m\n\u001b[1;32m    116\u001b[0m             output_tensor.view(-1).to(device) )\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m       \u001b[0mtotal_loss_batch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  translation(encoder=Encoder, decoder=Decoder, input_lang=input_lang, output_lang=output_lang, device=device)\n",
        "  response = input(\"do you want to translate other sentences? [Y/N]: \")\n",
        "  if response == \"N\":\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "id": "rVlrXOWf286b",
        "outputId": "a53079c1-ffe6-49f8-f219-26a6262d198a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type the sentence you want to translate:hello\n",
            "Original sentence = hello --> Translated sentence = ['saluti', '<EOS>']\n",
            "do you want to translate other sentences? [Y/N]: hi\n",
            "Type the sentence you want to translate:hi\n",
            "Original sentence = hi --> Translated sentence = ['salut√≤', '<EOS>']\n",
            "do you want to translate other sentences? [Y/N]: my name is tom\n",
            "Type the sentence you want to translate:my name is tom \n",
            "Original sentence = my name is tom  --> Translated sentence = ['il', 'mio', 'obiettivo', '<EOS>']\n",
            "do you want to translate other sentences? [Y/N]: y\n",
            "Type the sentence you want to translate:dog\n",
            "Original sentence = dog --> Translated sentence = ['il', 'cane', '<EOS>']\n",
            "do you want to translate other sentences? [Y/N]: y\n",
            "Type the sentence you want to translate:cat\n",
            "Original sentence = cat --> Translated sentence = ['gatto', 'grande', '<EOS>']\n",
            "do you want to translate other sentences? [Y/N]: sun \n",
            "Type the sentence you want to translate:sun\n",
            "Original sentence = sun --> Translated sentence = ['il', 'sole', '<EOS>']\n",
            "do you want to translate other sentences? [Y/N]: y\n",
            "Type the sentence you want to translate:moon\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-eec1fa5bd32f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtranslation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"do you want to translate other sentences? [Y/N]: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"N\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-913187b6662e>\u001b[0m in \u001b[0;36mtranslation\u001b[0;34m(encoder, decoder, input_lang, output_lang, device)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0minput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Type the sentence you want to translate:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_translation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-2283e132cf27>\u001b[0m in \u001b[0;36mstring_translation\u001b[0;34m(self, input_string)\u001b[0m\n\u001b[1;32m     96\u001b[0m                \u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WORD NOT FOUND, IMPOSSIBLE TO TRANSLATE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'matrix-processed.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'WORD NOT FOUND, IMPOSSIBLE TO TRANSLATE'"
          ]
        }
      ]
    }
  ]
}